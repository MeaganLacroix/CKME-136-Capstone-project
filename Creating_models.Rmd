---
title: "Creating_Models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
```

```{r}
train <- read.csv(file = "E:/CAPSTONE/train.csv", header = T, sep = ',')
test <- read.csv(file = "E:/CAPSTONE/test.csv", header = T, sep = ',')
```

```{r}
train <- train[-1]
test <- test[-1]
```

Using multiple linear regression with backward selection to choose the most important variables in a model where other_yes_per is the dependent variable. According to this model, the most relevant variables (having p < .05) are gender, attr_o, sinc_o, like_o, prob_o, tvsports, fun1_1, shar1_1, sinc2_1, attr3_1, fun3_1, subject_yes_per, and field_soft_sci. Approximately 73% of the variance in other_yes_per can be explained by its relationship to these variables. In this model, AIC was reduced from 1933.48 to 1870.78. 
```{r}
library(MASS)
full <- lm(other_yes_per ~., data = train)
stepB <- stepAIC(full, direction = "backward", trace = TRUE)
summary(stepB)
```

Determining variables of importance using an information gain method. The important variables found are attr_o, fun_o, shar_o, like_o, prob_o, and subject_yes_per. This finding shares some overlap with what was found with the linear model. 
```{r}
library(FSelector)
weights <- information.gain(other_yes_per~., train)
subset <- cutoff.k(weights, 20)
f <- as.simple.formula(subset, "other_yes_per")
print(f)
print(weights)
```

Determining the most important variables using Boruta. Important variables were found to be attr_o, like_o, shar_o, fun_o, prob_o, subject_yes_per, and intel_o. Moderately important variables are out_freq, race_asian, amb_o, and out_mod. This again shares some overlap with what was found with the linear model and also with what was found using information gain. Overall attr_o appears to be the most important variable across feature selection methods. 
```{r}
library(Boruta)
boruta_output <- Boruta(other_yes_per ~., data = train, pValue = .05, doTrace = 0)
boruta_sig <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_sig)

imps <- attStats(boruta_output)
imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]
head(imps2[order(-imps2$meanImp), ]) 
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance") 
```
Determining variable importance using rpart method.
```{r}
library(caret)
set.seed(100)
rpartmod <- train(other_yes_per ~., data = train, method = "rpart")
rpartimp <- varImp(rpartmod)
print(rpartimp)
```

Creating new datasets using variables identified as important
```{r}
train_new <- train[c(1, 2, 5, 7, 8, 9, 53, 54)]
test_new <- test[c(1, 2, 5, 7, 8, 9, 53, 54)]
```


Creating a tree model using the variables identified as important. 
```{r}
library(tree)
tree.date=tree(other_yes_per~., data = train_new)
summary(tree.date)
```

This tree shows that higher attr_o ratings (>= 6.49) and higher like_o ratings (>= 6.82) correspond to higher other_yes_per ratings. 
```{r}
plot(tree.date)
text(tree.date,pretty=0)
```
Calulating RMSE and MAE for the unpruned tree. 
```{r}
library(Metrics)
tree.date.pred <- predict(object = tree.date, newdata = test_new)
rmse(actual = test_new$other_yes_per, predicted = tree.date.pred)
mae(actual = test_new$other_yes_per, predicted = tree.date.pred)

```

10-fold cross-validation will now be used with the cv.tree function to determine the optimal level of tree complexity. The lowest cross-validation error ($dev) was for the tree with 8 node, suggesting that pruning this tree will not result in better performance. 
```{r}
set.seed(123)
cv.tree.date <- cv.tree(tree.date, FUN = prune.tree, K = 10)
cv.tree.date
```
```{r}
plot(cv.tree.date$size,cv.tree.date$dev,type='b')
```

Plotting a regression tree with the rpart package. This model is identical to the above tree model. 
```{r}
library(rpart)
library(rpart.plot)
set.seed(123)
rpart.date <- rpart(formula = other_yes_per ~., data = train_new, method = "anova")
rpart.plot(x = rpart.date, yesno = 2, type = 0, extra = 0)
summary(rpart.date)
```
The performance metrics for this tree are the same as the metrics for the above tree model. 
```{r}
rpart.pred <- predict(object = rpart.date, newdata = test_new)
rmse(actual = test_new$other_yes_per, predicted = rpart.pred)
mae(actual = test_new$other_yes_per, predicted = rpart.pred)

```
Tuning the hyperparameters of the rpart tree model
```{r}
plotcp(rpart.date)
print(rpart.date$cptable)
```


Getting optimal cp value based on cross-validation error
```{r}
opt_index <- which.min(rpart.date$cptable[ , "xerror"])
cp_opt <- rpart.date$cptable[opt_index, "CP"]
rpart.date.opt <- prune(tree = rpart.date, cp = cp_opt)
rpart.plot(x = rpart.date.opt, yesno = 2, type = 0, extra = 0)
```


RMSE is higher for this model, indicating poorer model performance. 
```{r}
rpart.date.opt.pred <- predict(object = rpart.date.opt, newdata = test_new)
rmse(actual = test_new$other_yes_per, predicted = rpart.date.opt.pred)
mae(actual = test_new$other_yes_per, predicted = rpart.date.opt.pred)
```


Generating a grid search of hyperparameter values
```{r}
splits <- seq(10, 100 , 10) #min number of data points required to attempt a split
depths <- seq(2, 8, 1) #depth of the decision tree
hyper_grid <- expand.grid(minsplit = splits, maxdepth = depths) # creating dataframe of all possible combos
print(hyper_grid)
```

```{r}
grid.search.models <- list() #creating empty list to store the models
```

Executing the grid search using a for loop
```{r}
 for(i in 1:nrow(hyper_grid)) {
   minsplit <- hyper_grid$minsplit[i]
   maxdepth <- hyper_grid$maxdepth[i]
   
   grid.search.models[[i]] <- rpart(formula = other_yes_per ~., 
                        data = train_new,
                        method = "anova",
                        minsplit = minsplit,
                        maxdepth = maxdepth)
 }
```

Compute the rmse values for each model
```{r}
rmse_values <- c() #creating empty vector to store rmse values
mae_values <- c()
for(i in 1:length(grid.search.models)) { #compute test rmse using for loop
  model <- grid.search.models[[i]] #retrieve ith model from the list
  grid.search.pred <- predict(object = model, #generate predictions
                              newdata = test_new)
  rmse_values[i] <- rmse(actual = test_new$other_yes_per, predicted = grid.search.pred) #compute test rmse and add to the vector
  mae_values[i] <- mae(actual = test_new$other_yes_per, predicted = grid.search.pred)
}
```

Finding the model with the smallest rmse. This model has the best (lowest) RMSE value of all the models tested so far. 
```{r}
best_rmse_model <- grid.search.models[[which.min(rmse_values)]]
best_rmse_model$control
rpart.plot(x = best_rmse_model, yesno = 2, type = 0, extra = 0)
print(min(rmse_values))

best_mae_model <- grid.search.models[[which.min(mae_values)]]
best_mae_model$control
rpart.plot(x = best_mae_model, yesno = 2, type = 0, extra = 0)
print(min(mae_values))

```


Building a random forest model using default values.  
```{r}
library(randomForest)
set.seed(123)
rf_model=randomForest(other_yes_per~., data= train_new, importance = TRUE)
rf_model

```
Calculating the RMSE and MAE values of this model. The random forest model shows the best performance of all the models tested so far. 
```{r}
rf_model_pred <- predict(object = rf_model, newdata = test_new)
date.test.yes <- test_new$other_yes_per
rmse(actual = test_new$other_yes_per, predicted = rf_model_pred)
mae(actual = test_new$other_yes_per, predicted = rf_model_pred)
```

Plotting the error per number of trees generated. 
```{r}
plot(rf_model_pred, date.test.yes)
abline(0,1)
plot(rf_model)
```
Estimating the importance of the variables in the random forest model. %IncMSE indicates the increase of the Mean Squared Error when given variable is randomly permuted. The most important variables appear to be like_o, and attr_o. Because IncNodePurity is biased in these models it will not be used to to make any decisions about importance. 
```{r}
importance(rf_model)
varImpPlot(rf_model)
```


Determining the best value of mtry for the random forest model using tune RF. It appears that mtry = 2 is the best option
```{r}
set.seed(1)
rf.mtry <- tuneRF(train_new[, -7], train_new[ , 7], ntreeTry = 50, stepFactor = 1, improve = 1e-5, doBest = FALSE)
rf.mtry

```

Testing a random forest model where mtry is set to 2 and ntree = 1000. This model shows the best performance so far. 
```{r}
set.seed(123)
rf_model2=randomForest(other_yes_per~., data= train_new, mtry = 2, ntree = 1000, importance = TRUE)
rf_model2

rf_model_pred2 <- predict(object = rf_model2, newdata = test_new)
rmse(actual = test_new$other_yes_per, predicted = rf_model_pred2)
mae(actual = test_new$other_yes_per, predicted = rf_model_pred2)

plot(rf_model_pred2, date.test.yes)
abline(0,1)
plot(rf_model2)
```

Creating a random forest model using the trainControl and train functions. The RMSE and MAE are higher for this model indicating poorer performance compared to the models above. 
```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
set.seed(246)
mtry <- sqrt(ncol(train_new))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(other_yes_per~., data=train_new, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_default)
```

Customizing a random forest grid search.
```{r}
metric<-'RMSE'

customRF <- list(type = "Regression", library = "randomForest", loop = NULL)

customRF$parameters <- data.frame(parameter = c("maxnodes", "ntree"), class = rep("numeric", 2), label = c("maxnodes", "ntree"))

customRF$grid <- function(x, y, len = NULL, search = "grid") {}

customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, maxnodes = param$maxnodes, ntree=param$ntree, ...)
}

customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
```

```{r}
# Set grid search parameters
control <- trainControl(method="repeatedcv", number=10, repeats=3, search='grid')

# Outline the grid of parameters
tunegrid <- expand.grid(.maxnodes=c(5, 6, 7, 8), .ntree=c(500, 1000, 2000))
set.seed(123)

# Train the model
rf_gridsearch <- train(other_yes_per~., data=train_new, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
```

The grid search has found the best model to be one with maxnodes = 8 and ntree = 2000. 
```{r}
plot(rf_gridsearch)
rf_gridsearch$bestTune
```


```{r}
rf_gridsearch_pred <- predict(object = rf_gridsearch, newdata = test_new)
rmse(actual = test_new$other_yes_per, predicted = rf_gridsearch_pred)
mae(actual = test_new$other_yes_per, predicted = rf_gridsearch_pred)
```

 


The RMSE and MAE values for this grid search model are higher than for the random forest model where mtry = 2 and ntree = 1000. 
```{r}
rf_model3=randomForest(other_yes_per~., data= train_new, mtry = 2, ntree = 2000, maxnodes = 8, importance = TRUE)
rf_model3

rf_model_pred3 <- predict(object = rf_model3, newdata = test_new)
rmse(actual = test_new$other_yes_per, predicted = rf_model_pred3)
mae(actual = test_new$other_yes_per, predicted = rf_model_pred3)

plot(rf_model_pred3, date.test.yes)
abline(0,1)
plot(rf_model3)
```



