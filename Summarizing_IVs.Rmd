---
title: "Summarizing the IVs"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
```


```{r}
train <- read.csv(file = "E:/CAPSTONE/train.csv", header = T, sep = ',')
test <- read.csv(file = "E:/CAPSTONE/test.csv", header = T, sep = ',')
```

Removing the Id variables for both datasets and the other_yes_group for the test dataset. 
```{r}
train <- train[-1:-2]
test <- test[c(-1:-2, -58)]
```

Computing a one-way ANOVA for the variables other_yes_per and gender. A significant result was found. Women have a higher mean score on the other_yes_per variable compared to men. 
```{r}
train %>%
   group_by(gender) %>%
   summarise(mean = mean(other_yes_per))

gender_aov <- aov(other_yes_per ~ gender, data = train)
summary(gender_aov)
cor.test(train$other_yes_per, train$gender, method = "pearson")
```

Analyzing the correlation between age and other_yes_per. A significant negative correlation was found. As age increases, number of yes responses decrease. 
```{r}
cor.test(train$age, train$other_yes_per, method = "pearson")
```


other_yes_per is highly correlated with all of the partner rating variables as well as the number of subject yes responses.
```{r}
library(Hmisc)
partnerattrvars <- train[c(2:9, 54:55)]
rcorr(as.matrix(partnerattrvars, type = "pearson"))
```


other_yes_per has a significant negative correlation with sinc1_1 and shar1_1. Subjects who value sincerity and shared interests in their partners receive fewer yes responses from partners. 
```{r}
attr1vars <- train[c(29:34, 54:55)]
rcorr(as.matrix(attr1vars, type = "pearson"))
```

other_yes_per is significantly positively correlated with attr2_1, and significantly negatively correlated with sinc2_1.
```{r}
attr2vars <- train[c(35:40, 54:55)]
rcorr(as.matrix(attr2vars, type = "pearson"))
```

other_yes_per is significantly positively correlated with attr3_1 and fun3_1. 
```{r}
attr3vars <- train[c(41:45, 54:55)]
rcorr(as.matrix(attr3vars, type = "pearson"))
```

other_yes_per is significantly positively correlated with sinc, intel and prob
```{r}
subject_attrvars <- train[46:55]
rcorr(as.matrix(subject_attrvars, type = "pearson"))
```

other_yes_per is significantly positively correlated with interests in exercise, hiking, shopping, and yoga, and negatively correlated with interests in tv.
```{r}
interestvars <- train[c(11:27, 54)]
rcorr(as.matrix(interestvars, type = "pearson"))
```

other_yes_per is not correlated with exphappy
```{r}
cor.test(train$exphappy, train$other_yes_per, method = "pearson")
```

other_yes_per is significantly negatively correlated with field_hard_sci, and positively correlated with field_bus_pol
```{r}
fieldvars <- train[c(56:60, 54)]
rcorr(as.matrix(fieldvars, type = "pearson"))
```

other_yes_per is significantly positively correlated with race_white and negatively correlated with race_asian. 

```{r}
racevars <- train[c(61:65, 54)]
rcorr(as.matrix(racevars, type = "pearson"))
```

other_yes_per is significantl positively correlated with date_freq and negatively correlated with date_infreq

```{r}
datevars <- train[c(66:69, 54)]
rcorr(as.matrix(datevars, type = "pearson"))
```

other_yes_per is significantly positively correlated with out_freq and negatively correlated with out_mod and out_infreq. 

```{r}
outvars <- train[c(70:72, 54)]
rcorr(as.matrix(outvars, type = "pearson"))
```
Creating a normalization function
```{r}
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))}
```

Normalizing the train dataset (was going to use this for KNN regression but then decided to use tree models instead)
```{r}
#train_n <- as.data.frame(lapply(train[2:55], normalize))
#train_n <- cbind(train_n, train[c(1, 56:72)])
```
Normalizing the test dataset (was going to use this for KNN regression but then decided to use tree models instead)
```{r}
#test_n <- as.data.frame(lapply(test[2:55], normalize))
#test_n <- cbind(test_n, test[c(1, 56:72)])
```
(was going to use this for KNN regression but then decided to use tree models instead)
```{r}
#write.csv(train_n, file = "E:/CAPSTONE/train_n.csv")
#write.csv(test_n, file = "E:/CAPSTONE/test_n.csv")
```

```{r}
write.csv(train, file = "E:/CAPSTONE/train_new.csv")
write.csv(test, file = "E:/CAPSTONE/test_new.csv")
```

